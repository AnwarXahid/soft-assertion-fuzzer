# ğŸ”ğŸ Soft Assertion Fuzzer

> **A next-generation fuzzing framework to expose the hidden dangers of numerical instability in ML applications.**
>  
> Automatically guided by *learned soft assertions*, this fuzzer finds the silent killers: NaNs, Infs, and **wrong-but-looks-fine predictions** in your ML models.  
> If it's unstable â€” we *will* break it. Intelligently.

---

![Soft Assertion Fuzzer Banner](https://github.com/AnwarXahid/soft-assertion-fuzzer/blob/main/soft-assertion-fuzzer-banner.png)
*â€œDetect what others miss. Fix what others ignore.â€*

---

## ğŸ“š What is This?

**Soft Assertion Fuzzer** is not your average testing tool.  
It fuses the power of **pretrained machine learning assertions** with the flexibility of **dynamic fuzzing**, enabling it to uncover deep numerical failures that plague ML applications â€” from instability in `exp()` & `log()` to subtle bugs in `matmul`& `relu`, etc.

- âœ¨ Targets PyTorch and TensorFlow-based ML code
- ğŸ§  Uses trained models to predict instability
- ğŸ“‰ Mutates inputs with gradient guidance â€” not just randomness
- ğŸ”¬ Validates failures with 6 oracles â€” not just NaN checks
- ğŸ“ˆ Outperforms 5 SOTA fuzzers on benchmarks and real-world bugs



> **Automatically Detecting Numerical Instability in Machine Learning Applications via Learned Soft Assertions**  
> ğŸ“œ [FSE 2025 Paper](https://arxiv.org/pdf/2504.15507) Â· ğŸ“¦ [Replication Package](https://figshare.com/s/6528d21ccd28bea94c32)
<!-- Short description with direct paper and dataset links -->


![Soft Assertion Fuzzer Illustration](https://github.com/AnwarXahid/soft-assertion-fuzzer/blob/main/soft-assertion-fuzzer-tool.png)
<!-- Insert your actual illustration URL above -->

*Illustration: Soft Assertions guide ML fuzzing to expose hidden numerical instabilities.*
<!-- Short caption explaining the image -->

---

---

## ğŸ“‹ Table of Contents

- [Prerequisites](#prerequisites)
- [Installation](#installation)
- [Quick Start](#quick-start)
- [Usage](#usage)
- [Configuration](#configuration)
- [Understanding Results](#understanding-results)
- [Examples](#examples)
- [Troubleshooting](#troubleshooting)
- [Contributing](#contributing)
- [Citation](#citation)
- [License](#license)

---

## ğŸ”§ Prerequisites

- Python 3.10+
- pip
- Git
- 4GB RAM minimum (8GB+ recommended)
- Works on Linux, macOS, Windows

---

## ğŸ› ï¸ Installation

```bash
git clone https://github.com/your-username/soft-assertion-fuzzer.git
cd soft-assertion-fuzzer
python3.10 -m venv venv
source venv/bin/activate  # Windows: venv\Scripts\activate
pip install -r requirements.txt
pip install -e .
```

Check installation:

```bash
softassertion-cli --help
```

> ğŸ“ Note:
> - Installation typically takes 2-5 minutes to complete. If you encounter "command not found" errors, please wait for the installation to finish completely, then restart your terminal or re-activate your virtual environment.

---

## ğŸ® Usage Modes

```bash
# Standard usage
softassertion-cli my_script.py

# Custom config
softassertion-cli my_script.py --config config/custom.yaml

# Verbose and timed
softassertion-cli my_script.py --verbose --timeout 60
```
---

## ğŸš€ Quick Start

Letâ€™s walk through using Soft Assertion Fuzzer on a real example.

### ğŸ§ª Step 1: Start with a simple ML script

Suppose you have a script named `test_saf.py`:

```python
# test_saf.py
import torch

x = torch.rand((3, 3)) - 0.5  # Some values < 0
y = torch.log(x)              # May trigger NaN due to negative input
print("Log output:\n", y)
```

This script **runs** â€” but silently generates `nan` values. No error is thrown.  
This is where **numerical instability** hides.

---

### âš™ï¸ Step 2: Instrument the script for fuzzing

Now, modify `test_saf.py` by adding Soft Assertion Fuzzer hooks:

```python
# test_saf.py (instrumented)
import torch
from softassertion.analysis.boundary_tracer import start_fuzz, end_fuzz

start_fuzz()  # Start fuzzing scope

x = torch.rand((3, 3)) - 0.5
y = torch.log(x)
print("Log output:\n", y)

end_fuzz()    # End fuzzing scope
```

> ğŸ§  `start_fuzz()` and `end_fuzz()` mark the **region of interest**.  
> Everything in between is monitored by the fuzzer â€” especially calls to known **unstable functions** like `log`, `exp`, `softmax`, etc.

---

### â–¶ï¸ Step 3: Run the fuzzer

Run Soft Assertion Fuzzer on your script:

```bash
softassertion-cli test_saf.py
```

Behind the scenes:

- It finds `torch.log()` as an unstable function.
- It queries the pretrained soft assertion model for `log`.
- It mutates the inputs using gradients (auto-diff).
- It triggers numerical instability (e.g., `nan`) based on oracle signals.
- It logs failure inputs, timestamps, and outputs.

---

### ğŸ“ Step 4: View results

All logs and reports are saved under:

```bash
experiments/logs/
```

Youâ€™ll find:

- `inputs_failed.npy` â€” inputs that triggered instability
- `summary_report.json` â€” what failed, why, how
- `trace.log` â€” fuzzer execution trace
---


## ğŸ§  How It Works

```text
ML Program
   â†“
[Hook Injection] â€” scans for unstable functions
   â†“
[Soft Assertion Model] â€” predicts direction to instability
   â†“
[Auto-Diff Engine] â€” mutates inputs with gradient signals
   â†“
[Oracle Checkers] â€” validates failure symptoms (NaN, wrong output, etc.)
   â†“
[Logger] â€” captures root causes, inputs, and summaries
```

---

## ğŸ§ª Evaluation

| Benchmark      | Bugs Found | Time (avg) |
|----------------|------------|------------|
| GRIST (79 apps) | âœ… 79/79   | â±ï¸ 0.646s  |
| Real-World (15 apps) | âœ… 12/15 | â±ï¸ 1.92s  |

> âœ³ï¸ Detected bugs missed by PyFuzz, Hypothesis, GRIST, Atheris, and RANUM.

---

## ğŸ” Sample Failure Report

```json
{
  "function": "torch.log",
  "input": [[-0.002, -0.531]],
  "oracle": "NaN/INF",
  "model_signal": "no_change",
  "severity": "high"
}
```

---

# ğŸ§© Project Structure

Below is a high-level overview of the **Soft Assertion Fuzzer** directory layout, highlighting key components of the project. This will help contributors and users understand where to find code, configurations, models, and results.

```text
.
â”œâ”€â”€ softassertion/                  # Core fuzzer logic and components
â”‚   â”œâ”€â”€ analysis/                   # AST parsing and hook injection
â”‚   â”œâ”€â”€ assertions/                 # Training pipeline and oracle logic
â”‚   â”œâ”€â”€ config/                     # Default configuration YAMLs
â”‚   â”œâ”€â”€ engine/                     # Fuzzing execution engine
â”‚   â”œâ”€â”€ fuzzing/                    # Autodiff-based mutators and history tracker
â”‚   â”œâ”€â”€ utils/                      # Shared utilities (logging, enums)
â”‚   â”œâ”€â”€ cli.py                      # Command-line entry point
â”‚   â””â”€â”€ main.py                     # Tool bootstrapper
â”‚
â”œâ”€â”€ oracles/                        # Function-specific numerical checkers
â”œâ”€â”€ generators/                     # Input generators (random, targeted)
â”œâ”€â”€ scripts/                        # Ready-to-fuzz ML scripts (15 real-world cases)
â”œâ”€â”€ experiments/                    # Logs and reports from fuzzing runs
â”‚   â”œâ”€â”€ bugs/                       # Confirmed bug-triggering cases
â”‚   â”œâ”€â”€ logs/                       # Fuzzer execution traces
â”‚   â””â”€â”€ reports/                    # Summaries and metrics
â”‚
â”œâ”€â”€ resources/                      # Pretrained models, datasets, figures
â”‚   â”œâ”€â”€ models/                     # Trained SA models for each function
â”‚   â”œâ”€â”€ dataset/                    # Evaluation results (XLSX)
â”‚   â””â”€â”€ images/                     # Visual diagrams used in docs
â”‚
â”œâ”€â”€ docs/                           # Project documentation
â”‚   â”œâ”€â”€ design.md                   # Architecture and internals
â”‚   â”œâ”€â”€ extend.md                   # How to add new functions
â”‚   â””â”€â”€ usage.md                    # Walkthrough and screenshots
â”‚
â”œâ”€â”€ tests/                          # Unit tests for major components
â”œâ”€â”€ run_models.py                   # Utility to train or test assertion models
â”œâ”€â”€ requirements.txt                # Python dependencies
â”œâ”€â”€ setup.py                        # Installation metadata
â”œâ”€â”€ LICENSE                         # MIT License
â”œâ”€â”€ README.md                       # Main project documentation
â””â”€â”€ soft-assertion-fuzzer-tool.png  # Tool illustration for README or docs
```

> ğŸ“ Note:
> - All real-world cases used in the FSE 2025 evaluation are located in `scripts/`.
> - Pretrained models (e.g., `exp_dataset_model_v100.pth`, `relu_model_rf.joblib`) are organized by input shape under `resources/models/`.
> - Additional diagrams and presentation assets live in `resources/images/`.


---

## ğŸ¤– Extend the Tool

Want to support your own function?

1. Write an oracle â†’ `oracles/oracle_myfunc.py`
2. Generate failure/pass dataset
3. Train a soft assertion model (RandomForestClassifier or others)
4. Plug into registry
5. Fuzz it. Crash it. Fix it.

---

## ğŸ¤ Contributing

```bash
pip install -r dev-requirements.txt
pre-commit install
pytest
```

---

## ğŸ“œ Citation

```bibtex
@inproceedings{sharmin2025automatically,
  title={Automatically Detecting Numerical Instability in Machine Learning Applications via Soft Assertions},
  author={Sharmin, Shaila and Zahid, Anwar Hossain and Bhattacharjee, Subhankar and Igwilo, Chiamaka and Kim, Miryung and Le, Wei},
  journal={arXiv preprint arXiv:2504.15507},
  year={2025}
}
```

---

## âš–ï¸ License

MIT License  
Â© 2025 Anwar Hossain Zahid, Iowa State University

---



