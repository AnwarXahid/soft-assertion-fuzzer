# ğŸ”ğŸ Soft Assertion Fuzzer

> **A next-generation fuzzing framework to expose the hidden dangers of numerical instability in ML applications.**
>  
> Automatically guided by *learned soft assertions*, this fuzzer finds the silent killers: NaNs, Infs, and **wrong-but-looks-fine predictions** in your ML models.  
> If it's unstable â€” we *will* break it. Intelligently.

---

![Soft Assertion Fuzzer Banner](https://github.com/AnwarXahid/soft-assertion-fuzzer/blob/main/soft-assertion-fuzzer-banner.png)
*â€œDetect what others miss. Fix what others ignore.â€*

---

## ğŸ“‹ Table of Contents

- [ğŸ“š What is This?](#what-is-this)
- [ğŸ“Œ Abstract](#abstract)
- [ğŸ¯ Motivation](#motivation)
- [ğŸš€ Key Features](#key-features)
- [ğŸ§  How It Works](#how-it-works)
- [ğŸ§ª Evaluation](#evaluation)
- [ğŸ”§ Prerequisites](#prerequisites)
- [ğŸ› ï¸ Installation](#installation)
- [ğŸ® Usage Modes](#usage-modes)
- [ğŸš€ Quick Start](#quick-start)
- [ğŸ§© Project Structure](#project-structure)
- [ğŸ¤– Extend the Tool](#extend-the-tool)
- [ğŸ¤ Contributing](#contributing)
- [ğŸ“œ Citation](#citation)
- [âš–ï¸ License](#license)

---

## ğŸ“š What is This?

**Soft Assertion Fuzzer** is not your average testing tool. It fuses the power of **pretrained machine learning assertions** with the flexibility of **dynamic fuzzing**, enabling it to uncover deep numerical failures that plague ML applications â€” from instability in `exp()` & `log()` to subtle bugs in `matmul`& `relu`, etc.

- âœ¨ Targets PyTorch and TensorFlow-based ML code
- ğŸ§  Uses trained models to predict instability
- ğŸ“‰ Mutates inputs with gradient guidance â€” not just randomness
- ğŸ”¬ Validates failures with 6 oracles â€” not just NaN checks
- ğŸ“ˆ Outperforms 5 SOTA fuzzers on benchmarks and real-world bugs



> **Automatically Detecting Numerical Instability in Machine Learning Applications via Learned Soft Assertions**  
> ğŸ“œ [FSE 2025 Paper](https://arxiv.org/pdf/2504.15507) Â· ğŸ“¦ [Replication Package](https://figshare.com/s/6528d21ccd28bea94c32)
<!-- Short description with direct paper and dataset links -->


![Soft Assertion Fuzzer Illustration](https://github.com/AnwarXahid/soft-assertion-fuzzer/blob/main/soft-assertion-fuzzer-tool.png)
<!-- Insert your actual illustration URL above -->

*Illustration: Soft Assertions guide ML fuzzing to expose hidden numerical instabilities.*
<!-- Short caption explaining the image -->

---

## ğŸ“Œ Abstract
<!-- Short, high-level summary of the project -->

**Soft Assertion Fuzzer** is an automated fuzz-testing tool designed specifically for Machine Learning (ML) applications. Leveraging pretrained ML models (termed *Soft Assertions*), it intelligently mutates inputs to trigger numerical instabilities such as NaNs, Infs, and silent incorrect outputs. Unlike conventional fuzzers, it captures subtle, domain-specific numerical errors and significantly outperforms existing state-of-the-art tools in finding critical numerical bugs.

---

## ğŸ¯ Motivation
<!-- Clearly explain the importance and need for this tool -->

Numerical instabilities are critical yet often overlooked problems in ML applications. Such instabilities can lead to incorrect predictions, wasted computational resources, or severe system failures.

**Limitations of existing tools:**

- âŒ Rely solely on random or coverage-based fuzzing.
- âŒ Miss nuanced numerical issues common in ML code.
- âŒ Unable to detect subtle bugs producing incorrect outputs.

**Soft Assertion Fuzzer addresses these issues by:**

- âœ… Learning numerical instability conditions from unit test data.
- âœ… Employing pretrained ML models for guided input mutation.
- âœ… Uncovering subtle bugs beyond mere crashes (NaN, Inf, incorrect predictions).

---

## ğŸš€ Key Features
<!-- Highlight distinct technical advantages clearly -->

- **ML-based Soft Assertions**: Automatically identifies numerical instability conditions.
- **Automatic Hook Injection**: Scans and integrates checkpoints within ML scripts.
- **Gradient-Based Mutation**: Guided input mutations based on ML insights.
- **Extensive Oracles**: Supports 6+ types of numerical error detections.
- **Detailed Failure Logging**: Tracks and logs all failure-inducing scenarios clearly.

---

## ğŸ§  How It Works

```text
ML Program
   â†“
[Hook Injection] â€” scans for unstable functions
   â†“
[Soft Assertion Model] â€” predicts direction to instability
   â†“
[Auto-Diff Engine] â€” mutates inputs with gradient signals
   â†“
[Oracle Checkers] â€” validates failure symptoms (NaN, wrong output, etc.)
   â†“
[Logger] â€” captures root causes, inputs, and summaries
```

---

## ğŸ§ª Evaluation

| Benchmark      | Bugs Found | Time (avg) |
|----------------|------------|------------|
| GRIST (79 apps) | âœ… 79/79   | â±ï¸ 0.646s  |
| Real-World (15 apps) | âœ… 12/15 | â±ï¸ 1.92s  |

> âœ³ï¸ Detected bugs missed by PyFuzz, Hypothesis, GRIST, Atheris, and RANUM.

---

## ğŸ”§ Prerequisites

- Python 3.10+
- pip
- Git
- 4GB RAM minimum (8GB+ recommended)
- Works on Linux, macOS, Windows

---

## ğŸ› ï¸ Installation

```bash
git clone https://github.com/your-username/soft-assertion-fuzzer.git
cd soft-assertion-fuzzer
python3.10 -m venv venv
source venv/bin/activate  # Windows: venv\Scripts\activate
pip install -r requirements.txt
pip install -e .
```

Check installation:

```bash
softassertion-cli --help
```

> ğŸ“ Note:
> - Installation typically takes 2-5 minutes to complete. If you encounter "command not found" errors, please wait for the installation to finish completely, then restart your terminal or re-activate your virtual environment.

---

## ğŸ® Usage Modes

```bash
# Standard usage
softassertion-cli my_script.py

# Custom config
softassertion-cli my_script.py --config config/custom.yaml

# Verbose and timed
softassertion-cli my_script.py --verbose --timeout 60
```
---

## ğŸš€ Quick Start

Letâ€™s walk through using Soft Assertion Fuzzer on a real example.

### ğŸ§ª Step 1: Start with a simple ML script

Suppose you have a script named `test_fuzzer.py`:

```python
# test_fuzzer.py
import torch

def test():
    
    x = torch.rand((3, 3))
    y = torch.nn.functional.softmax(x, dim=1)
    print("Softmax output:\n", y)

    z = torch.rand((3, 3)) - 0.5  
    w = torch.log(z)
    print("Log output:\n", w)

if __name__ == '__main__':
    test()
```

This script **runs** â€” but silently generates `nan` values. No error is thrown.  
This is where **numerical instability** hides.

---

### âš™ï¸ Step 2: Instrument the script for fuzzing

Now, modify `test_fuzzer.py` by adding Soft Assertion Fuzzer hooks:

```python
# test_fuzzer.py (instrumented)
import torch
from softassertion.analysis.boundary_tracer import start_fuzz, end_fuzz         # import hooks

def test():
    start_fuzz()               # Start fuzzing scope

    x = torch.rand((3, 3))
    y = torch.nn.functional.softmax(x, dim=1)
    print("Softmax output:\n", y)

    z = torch.rand((3, 3)) - 0.5  
    w = torch.log(z)
    print("Log output:\n", w)

    end_fuzz()                  # End fuzzing scope

if __name__ == '__main__':
    test()
```

> ğŸ§  `start_fuzz()` and `end_fuzz()` mark the **region of interest**. 
> Everything in between is monitored by the fuzzer â€” especially calls to known **unstable functions** like `log`, `exp`, `softmax`, etc.

---

### â–¶ï¸ Step 3: Run the fuzzer

Run Soft Assertion Fuzzer on your script:

```bash
softassertion-cli test_fuzzer.py
```

Behind the scenes:

- It finds `torch.log()` as an unstable function.
- It queries the pretrained soft assertion model for `log`.
- It mutates the inputs using gradients (auto-diff).
- It triggers numerical instability (e.g., `nan`) based on oracle signals.
- It logs failure inputs, timestamps, and outputs.

---

### ğŸ“ Step 4: View Results <!-- ğŸ”½ UPDATED SECTION START -->

All logs and reports are stored under:

```bash
experiments/logs/
```

Youâ€™ll find:

| File                          | Description                                 |
|-------------------------------|---------------------------------------------|
| `log_<function>.txt`          | Failure or timeout log for each function    |
| `summary_report.json`         | JSON summary of all results                 |
| `trace.log`                   | Execution timeline (step-by-step trace)     |
| `inputs_failed.npy` (optional)| Serialized failure-triggering inputs        |

> ğŸ§ª Example file: `experiments/logs/log_log.txt`

---

### ğŸ“ Step 5: Sample Failure Output

Hereâ€™s what a **real bug discovery** might look like:

#### ğŸ“¦ Console Output
```bash
[FuzzRunner] âŒ Bug triggered by input:
(tensor([[62.8839, 63.8055, 69.9051],
        [16.3082, 69.6523, -8.1343],
        [52.0661, 17.2648, 30.0087]], requires_grad=True),)
[FuzzRunner] âœ… Done: No failure found for: softmax

ğŸ“Š Case Study Summary:
  Total Cases: 2
  Passed: 1
  Failed: 1
  Average Time (sec): 5.0
  Failure Rate: 50.0
```

#### ğŸ“„ Summary JSON (`summary_report.json`)
```json
[
  {
    "function": "torch.log",
    "status": "FAIL",
    "oracle": "NaN/INF",
    "input": [[62.8839, 63.8055, 69.9051], [16.3082, 69.6523, -8.1343], [52.0661, 17.2648, 30.0087]],
    "model_signal": "no_change",
    "severity": "high",
    "elapsed_time": 5.0
  },
  {
    "function": "torch.softmax",
    "status": "PASS",
    "elapsed_time": 10.0
  }
]
```

---

## ğŸ§© Project Structure

Below is a high-level overview of the **Soft Assertion Fuzzer** directory layout, highlighting key components of the project. This will help contributors and users understand where to find code, configurations, models, and results.

```text
.
â”œâ”€â”€ softassertion/                  # Core fuzzer logic and components
â”‚   â”œâ”€â”€ analysis/                   # AST parsing and hook injection
â”‚   â”œâ”€â”€ assertions/                 # Training pipeline and oracle logic
â”‚   â”œâ”€â”€ config/                     # Default configuration YAMLs
â”‚   â”œâ”€â”€ engine/                     # Fuzzing execution engine
â”‚   â”œâ”€â”€ fuzzing/                    # Autodiff-based mutators and history tracker
â”‚   â”œâ”€â”€ utils/                      # Shared utilities (logging, enums)
â”‚   â”œâ”€â”€ cli.py                      # Command-line entry point
â”‚   â””â”€â”€ main.py                     # Tool bootstrapper
â”‚
â”œâ”€â”€ oracles/                        # Function-specific numerical checkers
â”œâ”€â”€ generators/                     # Input generators (random, targeted)
â”œâ”€â”€ scripts/                        # Ready-to-fuzz ML scripts (15 real-world cases)
â”œâ”€â”€ experiments/                    # Logs and reports from fuzzing runs
â”‚   â”œâ”€â”€ bugs/                       # Confirmed bug-triggering cases
â”‚   â”œâ”€â”€ logs/                       # Fuzzer execution traces
â”‚   â””â”€â”€ reports/                    # Summaries and metrics
â”‚
â”œâ”€â”€ resources/                      # Pretrained models, datasets, figures
â”‚   â”œâ”€â”€ models/                     # Trained SA models for each function
â”‚   â”œâ”€â”€ dataset/                    # Evaluation results (XLSX)
â”‚   â””â”€â”€ images/                     # Visual diagrams used in docs
â”‚
â”œâ”€â”€ docs/                           # Project documentation
â”‚   â”œâ”€â”€ design.md                   # Architecture and internals
â”‚   â”œâ”€â”€ extend.md                   # How to add new functions
â”‚   â””â”€â”€ usage.md                    # Walkthrough and screenshots
â”‚
â”œâ”€â”€ tests/                          # Unit tests for major components
â”œâ”€â”€ run_models.py                   # Utility to train or test assertion models
â”œâ”€â”€ requirements.txt                # Python dependencies
â”œâ”€â”€ setup.py                        # Installation metadata
â”œâ”€â”€ LICENSE                         # MIT License
â”œâ”€â”€ README.md                       # Main project documentation
â””â”€â”€ soft-assertion-fuzzer-tool.png  # Tool illustration for README or docs
```

> ğŸ“ Note:
> - All real-world cases used in the FSE 2025 evaluation are located in `scripts/`.
> - Pretrained models (e.g., `exp_dataset_model_v100.pth`, `relu_model_rf.joblib`) are organized by input shape under `resources/models/`.
> - Additional diagrams and presentation assets live in `resources/images/`.


---

## ğŸ¤– Extend the Tool

Want to support your own function?

1. Write an oracle â†’ `oracles/oracle_myfunc.py`
2. Generate failure/pass dataset
3. Train a soft assertion model (RandomForestClassifier or others)
4. Plug into registry
5. Fuzz it. Crash it. Fix it.

---

## ğŸ¤ Contributing

```bash
pip install -r dev-requirements.txt
pre-commit install
pytest
```

---

## ğŸ“œ Citation

```bibtex
@inproceedings{sharmin2025automatically,
  title={Automatically Detecting Numerical Instability in Machine Learning Applications via Soft Assertions},
  author={Sharmin, Shaila and Zahid, Anwar Hossain and Bhattacharjee, Subhankar and Igwilo, Chiamaka and Kim, Miryung and Le, Wei},
  journal={arXiv preprint arXiv:2504.15507},
  year={2025}
}
```

---

## âš–ï¸ License

MIT License  
Â© 2025 Anwar Hossain Zahid, Iowa State University

---



