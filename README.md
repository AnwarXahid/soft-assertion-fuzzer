# ğŸ”ğŸ Soft Assertion Fuzzer

> - **A smarter way to find numerical bugs in ML code.**
> - Powered by learned *soft assertions*, this fuzzer doesnâ€™t just catch crashes â€” it uncovers hidden instabilities like NaNs, Infs, and silent mispredictions that break model reliability.
> - If your ML code is numerically unstable â€” **we will detect it. Automatically. Precisely. At scale.**

---

![Soft Assertion Fuzzer Banner](https://github.com/AnwarXahid/soft-assertion-fuzzer/blob/main/soft-assertion-fuzzer-banner.png)
*â€œDetect what others miss. Fix what others ignore.â€*

---

## ğŸ“‹ Table of Contents

- [ğŸ“š What is This?](#what-is-this)
- [ğŸ“Œ Abstract](#abstract)
- [ğŸ¯ Motivation](#motivation)
- [ğŸš€ Key Features](#key-features)
- [ğŸ§  How It Works](#how-it-works)
- [ğŸ§ª Evaluation](#evaluation)
- [ğŸ”§ Prerequisites](#prerequisites)
- [ğŸ› ï¸ Installation](#installation)
- [ğŸ® Usage Modes](#usage-modes)
- [ğŸš€ Quick Start](#quick-start)
- [ğŸ§© Project Structure](#project-structure)
- [ğŸ¤– Extend the Tool](#extend-the-tool)
- [ğŸ¤ Contributing](#contributing)
- [ğŸ“œ Citation](#citation)
- [âš–ï¸ License](#license)

---

## ğŸ“š What Is This?

**Soft Assertion Fuzzer** is a precision fuzz-testing framework for detecting *numerical instability* in machine learning (ML) applications. It uniquely combines **pretrained ML-based assertions** with **gradient-guided input mutation**, enabling it to uncover bugs that silently corrupt outputs â€” such as NaNs, Infs, and **plausible-but-wrong predictions**.

Unlike conventional fuzzers, it doesn't rely on brute-force or coverage alone. Instead, it strategically navigates the input space using soft assertions â€” lightweight ML models trained to signal when and how numerical errors might occur.

> Designed for deep learning frameworks like **PyTorch** and **TensorFlow**, this tool is built to break brittle code with scientific precision.

**Highlights:**

- âœ¨ **Targets unstable math operations** like `exp()`, `log()`, `softmax()`, `matmul()`, etc.
- ğŸ§  **Uses pretrained ML models** to guide fuzzing toward failure-prone regions.
- ğŸ“‰ **Applies gradient-based mutation**, not just random noise.
- ğŸ”¬ **Validates failures using six oracles**, beyond simple NaN checks.
- ğŸ“ˆ **Outperforms five SOTA fuzzers** on benchmarks and real-world applications.

> ğŸ“„ **FSE 2025 Paper**:  
> _Automatically Detecting Numerical Instability in Machine Learning Applications via Learned Soft Assertions_  
> ğŸ“œ [Read on arXiv](https://arxiv.org/pdf/2504.15507) Â· ğŸ“¦ [Replication Package](https://figshare.com/s/6528d21ccd28bea94c32)

---

![Soft Assertion Fuzzer Illustration](https://github.com/AnwarXahid/soft-assertion-fuzzer/blob/main/soft-assertion-fuzzer-tool.png)  
*Illustration: Soft Assertions guide ML fuzzing to expose hidden numerical instabilities.*

---
## ğŸ“Œ Abstract

**Soft Assertion Fuzzer** is an automated testing framework tailored for detecting numerical instability in Machine Learning (ML) programs. Unlike traditional fuzzers that rely on random or syntactic mutations, this tool leverages *pretrained machine learning models*â€”called **Soft Assertions**â€”to identify instability-prone computations and guide input mutations toward failure-inducing conditions. It uncovers issues such as silent prediction errors, NaNs, and Infs in numerical code that are often missed by conventional testing tools.

---

## ğŸ¯ Motivation

Modern ML applications heavily rely on floating-point computations over large or sensitive numerical ranges. Small perturbations in input values or model weights can lead to severe instabilities, affecting both correctness and performance. However, such issues often go undetected during standard validation and deployment workflows.

**Key challenges in existing tools:**

- âŒ Limited to random input fuzzing or shallow heuristics.
- âŒ Lack domain-specific knowledge of numerical behavior in ML.
- âŒ Fail to detect non-crashing but semantically incorrect outputs.

**Soft Assertion Fuzzer is designed to overcome these by:**

- âœ… Learning failure-inducing behavior through supervised training on unit test data.
- âœ… Using ML-based classifiers (Soft Assertions) to guide gradient-informed mutations.
- âœ… Integrating multiple runtime oracles for fine-grained instability detection.

---

## ğŸš€ Key Features

- **Soft Assertions**: Pretrained models that predict instability regions for ML operators.
- **AST-Based Hooking**: Automatically instruments ML scripts for fuzzing.
- **Gradient-Guided Mutation**: Applies autodiff to refine input generation.
- **Oracle-Based Validation**: Employs semantic oracles (NaN/Inf, incorrect class, etc.).
- **Failure Tracing & Logging**: Captures inputs, timeouts, and full function traces.

---

## ğŸ§  How It Works

```text
ML Program
   â†“
[AST Scanner] â†’ Identifies unstable numerical functions
   â†“
[Soft Assertion Model] â†’ Predicts which inputs can trigger instability
   â†“
[Auto-Differentiation] â†’ Computes mutation directions via gradient signals
   â†“
[Oracle Evaluation] â†’ Validates outcome (e.g., NaN, Inf, semantic misclassification)
   â†“
[Structured Logging] â†’ Reports root cause, trigger inputs, and function call metadata
```

---

## ğŸ§ª Evaluation

| Dataset              | Programs | Bugs Found | Time/Program |
|----------------------|----------|------------|--------------|
| GRIST Benchmark      | 79       | âœ… 79/79    | â±ï¸ 0.646 sec |
| GitHub ML Projects   | 15       | âœ… 12/15    | â±ï¸ 1.92 sec  |

> ğŸ† Soft Assertion Fuzzer uncovered critical bugs missed by Hypothesis, PyFuzz, GRIST, Atheris, and RANUM.

---

## ğŸ”§ Prerequisites

- Python 3.10+
- pip
- Git
- 4GB RAM minimum (8GB+ recommended)
- Works on Linux, macOS, Windows

---

## ğŸ› ï¸ Installation

```bash
git clone https://github.com/your-username/soft-assertion-fuzzer.git
cd soft-assertion-fuzzer
python3.10 -m venv venv
source venv/bin/activate  # Windows: venv\Scripts\activate
pip install -r requirements.txt
pip install -e .
```

Check installation:

```bash
softassertion-cli --help
```

> ğŸ“ Note:
> - Installation typically takes 2-5 minutes to complete. If you encounter "command not found" errors, please wait for the installation to finish completely, then restart your terminal or re-activate your virtual environment.

---

## ğŸ® Usage Modes

```bash
# Standard usage
softassertion-cli my_script.py

# Custom config
softassertion-cli my_script.py --config config/custom.yaml

# Verbose and timed
softassertion-cli my_script.py --verbose --timeout 60
```
---

## ğŸš€ Quick Start

Letâ€™s walk through using Soft Assertion Fuzzer on a real example.

### ğŸ§ª Step 1: Start with a simple ML script

Suppose you have a script named `test_fuzzer.py`:

```python
# test_fuzzer.py
import torch

def test():
    
    x = torch.rand((3, 3))
    y = torch.nn.functional.softmax(x, dim=1)
    print("Softmax output:\n", y)

    z = torch.rand((3, 3)) - 0.5  
    w = torch.log(z)
    print("Log output:\n", w)

if __name__ == '__main__':
    test()
```

This script **runs** â€” but silently generates `nan` values. No error is thrown.  
This is where **numerical instability** hides.

---

### âš™ï¸ Step 2: Instrument the script for fuzzing

Now, modify `test_fuzzer.py` by adding Soft Assertion Fuzzer hooks:

```python
# test_fuzzer.py (instrumented)
import torch
from softassertion.analysis.boundary_tracer import start_fuzz, end_fuzz         # import hooks

def test():
    start_fuzz()               # Start fuzzing scope

    x = torch.rand((3, 3))
    y = torch.nn.functional.softmax(x, dim=1)
    print("Softmax output:\n", y)

    z = torch.rand((3, 3)) - 0.5  
    w = torch.log(z)
    print("Log output:\n", w)

    end_fuzz()                  # End fuzzing scope

if __name__ == '__main__':
    test()
```

> ğŸ§  `start_fuzz()` and `end_fuzz()` mark the **region of interest**. 
> Everything in between is monitored by the fuzzer â€” especially calls to known **unstable functions** like `log`, `exp`, `softmax`, etc.

---

### â–¶ï¸ Step 3: Run the fuzzer

Run Soft Assertion Fuzzer on your script:

```bash
softassertion-cli test_fuzzer.py
```

Behind the scenes:

- It finds `torch.log()` as an unstable function.
- It queries the pretrained soft assertion model for `log`.
- It mutates the inputs using gradients (auto-diff).
- It triggers numerical instability (e.g., `nan`) based on oracle signals.
- It logs failure inputs, timestamps, and outputs.

---

### ğŸ“ Step 4: View Results <!-- ğŸ”½ UPDATED SECTION START -->

All logs and reports are stored under:

```bash
experiments/logs/
```

Youâ€™ll find:

| File                          | Description                                 |
|-------------------------------|---------------------------------------------|
| `log_<function>.txt`          | Failure or timeout log for each function    |
| `summary_report.json`         | JSON summary of all results                 |
| `trace.log`                   | Execution timeline (step-by-step trace)     |
| `inputs_failed.npy` (optional)| Serialized failure-triggering inputs        |

> ğŸ§ª Example file: `experiments/logs/log_log.txt`

---

### ğŸ“ Step 5: Sample Failure Output

Hereâ€™s what a **real bug discovery** might look like:

#### ğŸ“¦ Console Output
```bash
[FuzzRunner] âŒ Bug triggered by input:
(tensor([[62.8839, 63.8055, 69.9051],
        [16.3082, 69.6523, -8.1343],
        [52.0661, 17.2648, 30.0087]], requires_grad=True),)
[FuzzRunner] âœ… Done: No failure found for: softmax

ğŸ“Š Case Study Summary:
  Total Cases: 2
  Passed: 1
  Failed: 1
  Average Time (sec): 5.0
  Failure Rate: 50.0
```

#### ğŸ“„ Summary JSON (`summary_report.json`)
```json
[
  {
    "function": "torch.log",
    "status": "FAIL",
    "oracle": "NaN/INF",
    "input": [[62.8839, 63.8055, 69.9051], [16.3082, 69.6523, -8.1343], [52.0661, 17.2648, 30.0087]],
    "model_signal": "no_change",
    "severity": "high",
    "elapsed_time": 5.0
  },
  {
    "function": "torch.softmax",
    "status": "PASS",
    "elapsed_time": 10.0
  }
]
```

---

## ğŸ§© Project Structure

Below is a high-level overview of the **Soft Assertion Fuzzer** directory layout, highlighting key components of the project. This will help contributors and users understand where to find code, configurations, models, and results.

```text
.
â”œâ”€â”€ softassertion/                  # Core fuzzer logic and components
â”‚   â”œâ”€â”€ analysis/                   # AST parsing and hook injection
â”‚   â”œâ”€â”€ assertions/                 # Training pipeline and oracle logic
â”‚   â”œâ”€â”€ config/                     # Default configuration YAMLs
â”‚   â”œâ”€â”€ engine/                     # Fuzzing execution engine
â”‚   â”œâ”€â”€ fuzzing/                    # Autodiff-based mutators and history tracker
â”‚   â”œâ”€â”€ utils/                      # Shared utilities (logging, enums)
â”‚   â”œâ”€â”€ cli.py                      # Command-line entry point
â”‚   â””â”€â”€ main.py                     # Tool bootstrapper
â”‚
â”œâ”€â”€ oracles/                        # Function-specific numerical checkers
â”œâ”€â”€ generators/                     # Input generators (random, targeted)
â”œâ”€â”€ scripts/                        # Ready-to-fuzz ML scripts (15 real-world cases)
â”œâ”€â”€ experiments/                    # Logs and reports from fuzzing runs
â”‚   â”œâ”€â”€ bugs/                       # Confirmed bug-triggering cases
â”‚   â”œâ”€â”€ logs/                       # Fuzzer execution traces
â”‚   â””â”€â”€ reports/                    # Summaries and metrics
â”‚
â”œâ”€â”€ resources/                      # Pretrained models, datasets, figures
â”‚   â”œâ”€â”€ models/                     # Trained SA models for each function
â”‚   â”œâ”€â”€ dataset/                    # Evaluation results (XLSX)
â”‚   â””â”€â”€ images/                     # Visual diagrams used in docs
â”‚
â”œâ”€â”€ docs/                           # Project documentation
â”‚   â”œâ”€â”€ design.md                   # Architecture and internals
â”‚   â”œâ”€â”€ extend.md                   # How to add new functions
â”‚   â””â”€â”€ usage.md                    # Walkthrough and screenshots
â”‚
â”œâ”€â”€ tests/                          # Unit tests for major components
â”œâ”€â”€ run_models.py                   # Utility to train or test assertion models
â”œâ”€â”€ requirements.txt                # Python dependencies
â”œâ”€â”€ setup.py                        # Installation metadata
â”œâ”€â”€ LICENSE                         # MIT License
â”œâ”€â”€ README.md                       # Main project documentation
â””â”€â”€ soft-assertion-fuzzer-tool.png  # Tool illustration for README or docs
```

> ğŸ“ Note:
> - All real-world cases used in the FSE 2025 evaluation are located in `scripts/`.
> - Pretrained models (e.g., `exp_dataset_model_v100.pth`, `relu_model_rf.joblib`) are organized by input shape under `resources/models/`.
> - Additional diagrams and presentation assets live in `resources/images/`.


---

## ğŸ¤– Extend the Tool

Want to support your own function?

1. Write an oracle â†’ `oracles/oracle_myfunc.py`
2. Generate failure/pass dataset
3. Train a soft assertion model (RandomForestClassifier or others)
4. Plug into registry
5. Fuzz it. Crash it. Fix it.

---

## ğŸ¤ Contributing

```bash
pip install -r dev-requirements.txt
pre-commit install
pytest
```

---

## ğŸ“œ Citation

```bibtex
@inproceedings{sharmin2025automatically,
  title={Automatically Detecting Numerical Instability in Machine Learning Applications via Soft Assertions},
  author={Sharmin, Shaila and Zahid, Anwar Hossain and Bhattacharjee, Subhankar and Igwilo, Chiamaka and Kim, Miryung and Le, Wei},
  journal={arXiv preprint arXiv:2504.15507},
  year={2025}
}
```

---

## âš–ï¸ License

MIT License  
Â© 2025 Anwar Hossain Zahid, Iowa State University

---



